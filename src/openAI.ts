import { Configuration, OpenAIApi } from 'openai'

const OPENAI_API_KEY = process.env.OPENAI_API_KEY
const OPEN_AI_MODEL = "gpt-3.5-turbo"

const configuration = new Configuration({
    apiKey: OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);


/**
 * getCompletion uses the OpenAI API to generate a response to a prompt
 * @param prompt the prompt to generate a response to
 * @param systemPromptText the system prompt to use, if not provided, a default system prompt will be used
 * @returns the response generated by the OpenAI API
 * @throws an error if the OpenAI API key is not configured
 * @throws an error if the OpenAI API request fails
**/
export const getCompletion = async function (prompt, systemPromptText = '') {
    if (!OPENAI_API_KEY) {
        console.error("OpenAI API key not configured, please follow instructions in README.md")
        return
    }
    try {
        // format the date as Mon-dd-yyyy
        const date = new Date()
        const month = date.toLocaleString('default', { month: 'short' })
        const day = date.getDate()
        const year = date.getFullYear()
        const formatted_date = `${month}-${day}-${year}`

        // add a system prompt to the prompt
        let systemPrompt = { role: "system", content: `You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible. Knowledge cutoff Current date: ${formatted_date}` }

        // allow the system prompt to be overridden
        if(systemPromptText){
            systemPrompt = { role: "system", content: systemPromptText }
        } 

        // combine the system prompt with the user prompt in an array
        const combinedPrompt = [systemPrompt, ...prompt]


        // format the prompt
        const completion = await openai.createChatCompletion({
            model: OPEN_AI_MODEL,
            messages: combinedPrompt,
            // the temperature parameter controls the randomness of the response
            // the higher the temperature, the more random the response
            // the lower the temperature, the more likely the response will be a continuation of the prompt
            temperature: 0.7,

        });

        let reply = completion?.data?.choices[0]?.message?.content.trim()


        return reply
    } catch (error: any) {

        if (error.response) {
            console.error("Error with OpenAI API request", error.response.status, error.response.data);

        } else {
            console.error(`Error with OpenAI API request: ${error.message}`);
        }
        return error
    }
}